{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/cz2064/envs/dl4med/lib/python3.6/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = '/scratch/cz2064/myjupyter/Time_Series/Data/data_VoxCeleb/wav/id10983/K3VF9KATPqc/00123.wav'\n",
    "sample_path_2 = '/scratch/cz2064/myjupyter/Time_Series/Data/data_VoxCeleb/wav/id10452/cLMPZ3fQHJw/00004.wav'\n",
    "sample,sample_rate = torchaudio.load(sample_path_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "sample,sample_rate = torchaudio.load(sample_path)\n",
    "torchaudio.save('2_style.wav',sample,sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "sample,sample_rate = torchaudio.load(sample_path_2)\n",
    "torchaudio.save('2_content.wav',sample,sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_mean = 2e-05\n",
    "statistical_std = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_normalization(sample_path,sampling_length=256*256,mean = statistical_mean, std = statistical_std):\n",
    "    sample,_ = torchaudio.load(sample_path)\n",
    "    length = sample.size(1)\n",
    "    if length<sampling_length:\n",
    "        pad = int(sampling_length-length)\n",
    "        sample = torch.cat((sample,torch.zeros((1,pad))),-1)\n",
    "    sample = sample[:,:sampling_length]\n",
    "    sample = (sample-mean)/std\n",
    "    sample = sample.unsqueeze(0)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_normalization(tensor,mean = statistical_mean, std = statistical_std):\n",
    "    audio = tensor.cpu().clone()\n",
    "    audio = audio.squeeze(0)\n",
    "    audio = (audio * std) + mean\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = \"2_style.wav\"\n",
    "content = \"2_content.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1,_ = torchaudio.load(style)\n",
    "lenth_1 = sample_1.size(1)\n",
    "sample_2,_ = torchaudio.load(content)\n",
    "lenth_2 = sample_2.size(1)\n",
    "lenth = lenth_2#max(lenth_1,lenth_2)\n",
    "\n",
    "\n",
    "style_img = load_and_normalization(style,lenth).to(device)\n",
    "content_img = load_and_normalization(content,lenth).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biden and Trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "biden_sample,_ = torchaudio.load('Biden.wav')\n",
    "downsample_resample = torchaudio.transforms.Resample(_, sample_rate)\n",
    "biden_sample = downsample_resample(biden_sample)\n",
    "biden_sample = biden_sample[0].unsqueeze(0)\n",
    "lenth_1 = biden_sample.size(1)\n",
    "\n",
    "trump_sample,_ = torchaudio.load('Trump_short.wav')\n",
    "downsample_resample = torchaudio.transforms.Resample(_, sample_rate)\n",
    "trump_sample = downsample_resample(trump_sample)\n",
    "trump_sample = trump_sample[0].unsqueeze(0)\n",
    "lenth_2 = trump_sample.size(1)\n",
    "\n",
    "lenth = max(lenth_1,lenth_2)\n",
    "\n",
    "def load_and_normalization(sample,sampling_length=lenth,mean = statistical_mean, std = statistical_std):\n",
    "    length = sample.size(1)\n",
    "    if length<sampling_length:\n",
    "        pad = int(sampling_length-length)\n",
    "        sample = torch.cat((sample,torch.zeros((1,pad))),-1)\n",
    "    sample = sample[:,:sampling_length]\n",
    "    sample = (sample-mean)/std\n",
    "    sample = sample.unsqueeze(0)\n",
    "    return sample\n",
    "\n",
    "style_img = load_and_normalization(trump_sample).to(device)\n",
    "content_img = load_and_normalization(biden_sample).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target,):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input):\n",
    "    a, b, c = input.size()\n",
    "    features = input.view(a * b, c )\n",
    "    G = torch.mm(features, features.t())\n",
    "    return G.div(a * b * c )\n",
    "\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG_1D(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG_1D, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        #self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        #out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 1\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool1d(kernel_size=5, stride=5)]\n",
    "            else:\n",
    "                layers += [nn.Conv1d(in_channels, x, kernel_size=10, padding=5),\n",
    "                           nn.BatchNorm1d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool1d(21)]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "model_CNN = VGG_1D('VGG16')\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.cnn = model_CNN\n",
    "        self.fc1 = nn.Linear(512*3,2048)\n",
    "        self.activation_fc1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(2048,1024)\n",
    "        self.activation_fc2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(1024,2)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.cnn(x1)\n",
    "        x2 = self.cnn(x2)\n",
    "        \n",
    "        x_add = x1+x2\n",
    "        x_minus = x1-x2\n",
    "        x_multiply = x1*x2\n",
    "        x = torch.cat((x_add, x_minus, x_multiply),-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation_fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "check_path = '/scratch/cz2064/myjupyter/Time_Series/notebook/python_files/Model_CNN_VGG16/Second_Train/checkpoint_CNN.pt'\n",
    "model.load_state_dict(torch.load(check_path,map_location=torch.device('cpu'))['best_model_wts'])\n",
    "pretrained_VGG  = model.cnn.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = nn.Sequential()\n",
    "i = 0\n",
    "for layer in pretrained_VGG.children():\n",
    "    if isinstance(layer, nn.Conv1d):\n",
    "        cnn.add_module(str(i),layer)\n",
    "    elif isinstance(layer, nn.ReLU):\n",
    "        cnn.add_module(str(i),layer)\n",
    "    elif isinstance(layer, nn.MaxPool1d):\n",
    "        cnn.add_module(str(i),layer)\n",
    "    elif isinstance(layer, nn.BatchNorm1d):\n",
    "        cnn.add_module(str(i),layer)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean=statistical_mean, std=statistical_std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = torch.tensor(mean).to(device)\n",
    "        self.std = torch.tensor(std).to(device)\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers_default = ['conv_1']\n",
    "style_layers_default = ['conv_1','conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "def get_style_model_and_losses(cnn,style_img, content_img,\n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "\n",
    "    normalization = Normalization().to(device)\n",
    "\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential\n",
    "    # to put in modules that are supposed to be activated sequentially\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    i = 0  # increment every time we see a conv\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv1d):\n",
    "            i += 1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            # The in-place version doesn't play very nicely with the ContentLoss\n",
    "            # and StyleLoss we insert below. So we replace with out-of-place\n",
    "            # ones here.\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool1d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm1d):\n",
    "            name = 'bn_{}'.format(i)\n",
    "        else:\n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            # add content loss:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            # add style loss:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # now we trim off the layers after the last content and style losses\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "\n",
    "    model = model[:(i + 1)]\n",
    "\n",
    "    return model, style_losses, content_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = content_img.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_optimizer(input_img):\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_style_transfer(cnn,content_img, style_img, input_img, num_steps=300,\n",
    "                       style_weight=1000000, content_weight=1):\n",
    "    \"\"\"Run the style transfer.\"\"\"\n",
    "    print('Building the style transfer model..')\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn, style_img, content_img)\n",
    "    optimizer = get_input_optimizer(input_img)\n",
    "\n",
    "    print('Optimizing..')\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            # correct the values of updated input image\n",
    "            #input_img.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.loss\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.loss\n",
    "\n",
    "            style_score *= style_weight\n",
    "            content_score *= content_weight\n",
    "\n",
    "            loss = style_score + content_score\n",
    "            loss.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 1000 == 0:\n",
    "                print(\"run {}:\".format(run))\n",
    "                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
    "                    style_score.item(), content_score.item()))\n",
    "                print()\n",
    "\n",
    "            return style_score + content_score\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # a last correction...\n",
    "    #input_img.data.clamp_(0, 1)\n",
    "\n",
    "    return input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [1000]:\n",
      "Style Loss : 3941.174316 Content Loss: 60.644741\n",
      "\n",
      "run [2000]:\n",
      "Style Loss : 824.923279 Content Loss: 64.474380\n",
      "\n",
      "run [3000]:\n",
      "Style Loss : 348.591949 Content Loss: 66.360519\n",
      "\n",
      "run [4000]:\n",
      "Style Loss : 189.707794 Content Loss: 67.834724\n",
      "\n",
      "run [5000]:\n",
      "Style Loss : 121.392448 Content Loss: 68.992905\n",
      "\n",
      "run [6000]:\n",
      "Style Loss : 86.886482 Content Loss: 69.812057\n",
      "\n",
      "run [7000]:\n",
      "Style Loss : 67.194351 Content Loss: 70.365768\n",
      "\n",
      "run [8000]:\n",
      "Style Loss : 54.620529 Content Loss: 70.751869\n",
      "\n",
      "run [9000]:\n",
      "Style Loss : 46.002888 Content Loss: 71.037300\n",
      "\n",
      "run [10000]:\n",
      "Style Loss : 39.794888 Content Loss: 71.256752\n",
      "\n",
      "run [11000]:\n",
      "Style Loss : 35.027843 Content Loss: 71.422020\n",
      "\n",
      "run [12000]:\n",
      "Style Loss : 31.396433 Content Loss: 71.540009\n",
      "\n",
      "run [13000]:\n",
      "Style Loss : 28.500126 Content Loss: 71.623276\n",
      "\n",
      "run [14000]:\n",
      "Style Loss : 26.138916 Content Loss: 71.673767\n",
      "\n",
      "run [15000]:\n",
      "Style Loss : 24.176781 Content Loss: 71.702164\n",
      "\n",
      "run [16000]:\n",
      "Style Loss : 22.534859 Content Loss: 71.710083\n",
      "\n",
      "run [17000]:\n",
      "Style Loss : 21.150162 Content Loss: 71.703781\n",
      "\n",
      "run [18000]:\n",
      "Style Loss : 19.969248 Content Loss: 71.683853\n",
      "\n",
      "run [19000]:\n",
      "Style Loss : 18.953230 Content Loss: 71.655968\n",
      "\n",
      "run [20000]:\n",
      "Style Loss : 18.052893 Content Loss: 71.624008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = run_style_transfer(cnn,content_img, style_img, input_img,num_steps=20000,style_weight=1e10, content_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1540, -0.1254,  0.1124, -0.0159,  0.0037,  0.0395, -0.1904, -0.2021,\n",
       "        -0.2771, -0.2235], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0,0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_audio = inverse_normalization(output)\n",
    "torchaudio.save('2_output.wav',output_audio,sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 75521])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_img.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0509, -0.0218, -0.2464,  ..., -0.3025, -1.3004, -1.2815]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import wave\n",
    "import nextpow2\n",
    "import math\n",
    "\n",
    "# 打开WAV文档\n",
    "f = wave.open(\"BT_output.wav\")\n",
    "# 读取格式信息\n",
    "# (nchannels, sampwidth, framerate, nframes, comptype, compname)\n",
    "params = f.getparams()\n",
    "nchannels, sampwidth, framerate, nframes = params[:4]\n",
    "fs = framerate\n",
    "# 读取波形数据\n",
    "str_data = f.readframes(nframes)\n",
    "f.close()\n",
    "# 将波形数据转换为数组\n",
    "x = np.fromstring(str_data, dtype=np.short)\n",
    "# 计算参数\n",
    "len_ = 20 * fs // 1000 # 样本中帧的大小\n",
    "PERC = 50 # 窗口重叠占帧的百分比\n",
    "len1 = len_ * PERC // 100  # 重叠窗口\n",
    "len2 = len_ - len1   # 非重叠窗口\n",
    "# 设置默认参数\n",
    "Thres = 3\n",
    "Expnt = 2.0\n",
    "beta = 0.002\n",
    "G = 0.9\n",
    "# 初始化汉明窗\n",
    "win = np.hamming(len_)\n",
    "# normalization gain for overlap+add with 50% overlap\n",
    "winGain = len2 / sum(win)\n",
    "\n",
    "# Noise magnitude calculations - assuming that the first 5 frames is noise/silence\n",
    "nFFT = 2 * 2 ** (nextpow2.nextpow2(len_))\n",
    "noise_mean = np.zeros(nFFT)\n",
    "\n",
    "j = 0\n",
    "for k in range(1, 6):\n",
    "    noise_mean = noise_mean + abs(np.fft.fft(win * x[j:j + len_], nFFT))\n",
    "    j = j + len_\n",
    "noise_mu = noise_mean / 5\n",
    "\n",
    "# --- allocate memory and initialize various variables\n",
    "k = 1\n",
    "img = 1j\n",
    "x_old = np.zeros(len1)\n",
    "Nframes = len(x) // len2 - 1\n",
    "xfinal = np.zeros(Nframes * len2)\n",
    "\n",
    "# =========================    Start Processing   ===============================\n",
    "for n in range(0, Nframes):\n",
    "    # Windowing\n",
    "    insign = win * x[k-1:k + len_ - 1]\n",
    "    # compute fourier transform of a frame\n",
    "    spec = np.fft.fft(insign, nFFT)\n",
    "    # compute the magnitude\n",
    "    sig = abs(spec)\n",
    "\n",
    "    # save the noisy phase information\n",
    "    theta = np.angle(spec)\n",
    "    SNRseg = 10 * np.log10(np.linalg.norm(sig, 2) ** 2 / np.linalg.norm(noise_mu, 2) ** 2)\n",
    "\n",
    "\n",
    "    def berouti(SNR):\n",
    "        if -5.0 <= SNR <= 20.0:\n",
    "            a = 4 - SNR * 3 / 20\n",
    "        else:\n",
    "            if SNR < -5.0:\n",
    "                a = 5\n",
    "            if SNR > 20:\n",
    "                a = 1\n",
    "        return a\n",
    "\n",
    "\n",
    "    def berouti1(SNR):\n",
    "        if -5.0 <= SNR <= 20.0:\n",
    "            a = 3 - SNR * 2 / 20\n",
    "        else:\n",
    "            if SNR < -5.0:\n",
    "                a = 4\n",
    "            if SNR > 20:\n",
    "                a = 1\n",
    "        return a\n",
    "\n",
    "    if Expnt == 1.0:  # 幅度谱\n",
    "        alpha = berouti1(SNRseg)\n",
    "    else:  # 功率谱\n",
    "        alpha = berouti(SNRseg)\n",
    "    #############\n",
    "    sub_speech = sig ** Expnt - alpha * noise_mu ** Expnt;\n",
    "    # 当纯净信号小于噪声信号的功率时\n",
    "    diffw = sub_speech - beta * noise_mu ** Expnt\n",
    "    # beta negative components\n",
    "\n",
    "    def find_index(x_list):\n",
    "        index_list = []\n",
    "        for i in range(len(x_list)):\n",
    "            if x_list[i] < 0:\n",
    "                index_list.append(i)\n",
    "        return index_list\n",
    "\n",
    "    z = find_index(diffw)\n",
    "    if len(z) > 0:\n",
    "        # 用估计出来的噪声信号表示下限值\n",
    "        sub_speech[z] = beta * noise_mu[z] ** Expnt\n",
    "        # --- implement a simple VAD detector --------------\n",
    "    if SNRseg < Thres:  # Update noise spectrum\n",
    "        noise_temp = G * noise_mu ** Expnt + (1 - G) * sig ** Expnt  # 平滑处理噪声功率谱\n",
    "        noise_mu = noise_temp ** (1 / Expnt)  # 新的噪声幅度谱\n",
    "    # flipud函数实现矩阵的上下翻转，是以矩阵的“水平中线”为对称轴\n",
    "    # 交换上下对称元素\n",
    "    sub_speech[nFFT // 2 + 1:nFFT] = np.flipud(sub_speech[1:nFFT // 2])\n",
    "    x_phase = (sub_speech ** (1 / Expnt)) * (np.array([math.cos(x) for x in theta]) + img * (np.array([math.sin(x) for x in theta])))\n",
    "    # take the IFFT\n",
    "\n",
    "    xi = np.fft.ifft(x_phase).real\n",
    "    # --- Overlap and add ---------------\n",
    "    xfinal[k-1:k + len2 - 1] = x_old + xi[0:len1]\n",
    "    x_old = xi[0 + len1:len_]\n",
    "    k = k + len2\n",
    "# 保存文件\n",
    "wf = wave.open('en_outfile.wav', 'wb')\n",
    "# 设置参数\n",
    "wf.setparams(params)\n",
    "# 设置波形文件 .tostring()将array转换为data\n",
    "wave_data = (winGain * xfinal).astype(np.short)\n",
    "wf.writeframes(wave_data.tostring())\n",
    "wf.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
